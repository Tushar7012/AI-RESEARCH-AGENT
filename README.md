# Patent Innovation Predictor â€” Local RAG Agent
This project is a local Research Agent for analyzing patents stored as PDFs, using:

LangChain, OpenSearch, Ollama, and CrewAI

Your own local embeddings

No paid API keys â€” runs fully on your machine

# âš™ï¸ How It Works
1ï¸âƒ£ You load PDF patents

Store your PDF files in a folder (e.g. ./data/patent_pdfs)

Use ingestion.py to:

    Load each PDF

    Split into text chunks

    Generate embeddings using nomic-embed-text (or your Ollama embedding model)

    Store all chunks + embeddings in OpenSearch

2ï¸âƒ£ You query the PDFs

Use search_tools.py to run:

    ğŸ” Keyword Search â€” match plain text

    ğŸ§² Semantic Search â€” vector similarity

    ğŸ§© Hybrid Search â€” both together

    ğŸ” Iterative Search â€” refine and explore

3ï¸âƒ£ You run an Agent

Uses CrewAI with:

    Multiple agents (analyst, reviewer, trend summarizer)

    Uses your Ollama model for LLM tasks (llama3, mistral, deepseek-llm, etc.)

    Plans research, finds chunks, summarizes insights

4ï¸âƒ£ Optional Frontend

You can build a simple Streamlit or Flask app to:

    Upload new PDFs

    Run searches

    Show results in your browser

# ğŸ“¦ Project Structure
    .
    â”œâ”€â”€ data/patent_pdfs/          # Your PDF files
    â”œâ”€â”€ Agent/
    â”‚   â”œâ”€â”€ ingestion.py           # Loads + splits + embeds
    â”‚   â”œâ”€â”€ vectors/embedding.py   # Embedding logic
    â”‚   â”œâ”€â”€ search_client/         # OpenSearch config
    â”‚   â”œâ”€â”€ search_tools.py        # Keyword + semantic + hybrid
    â”‚   â”œâ”€â”€ crew_ai/               # CrewAI multi-agent workflow
    â”‚   â””â”€â”€ main_menu.py           # Main CLI menu
    â”œâ”€â”€ docker-compose.yml         # Runs OpenSearch + Dashboard

# How To Run
Step 1: Start OpenSearch

    docker compose -f docker-compose.yml up

Step 2: Run the PDF ingestion

    python Agent/ingestion.py

Step 3: Run the agent or search tools

    python Agent/crew_ai/patent_crew.py

Step 4: For embeddings â€” run ollama serve and pull a model:

    ollama pull llama3
    ollama serve

# ğŸš€ Tips
    Make sure OpenSearch is running (http://localhost:9200)

    Make sure your Ollama model is running (ollama list)

    Use hybrid search for best results!

    Add more PDFs anytime â€” reâ€‘ingest to update.

# TechStack
    LangChain
    OpenSearch
    Ollama
    CrewAI
    Embeddings